
storage.sources  = kafka
storage.channels = memory
storage.sinks    = hdfs,elasticsearch

storage.sources.kafka.type = org.apache.flume.source.kafka.KafkaSource
storage.sources.kafka.zookeeperConnect = shapira-1:2181
storage.sources.kafka.topic = shapira
storage.sources.kafka.groupId = flume
storage.sources.kafka.channels = channel1
storage.sources.kafka.interceptors = i1
storage.sources.kafka.interceptors.i1.type = timestamp
storage.sources.kafka.kafka.consumer.timeout.ms = 100

storage.channels.memory.type   = memory
storage.channels.memory.capacity = 10000
storage.channels.memory.transactionCapacity = 1000

storage.sinks.hdfs.type         = hdfs
storage.sinks.hdfs.hdfs.path    = /tmp/shapira/kafka/%{topic}/%y-%m-%d
storage.sinks.hdfs.hdfs.rollInterval = 5
storage.sinks.hdfs.hdfs.rollSize = 0
storage.sinks.hdfs.hdfs.rollCount = 0
storage.sinks.hdfs.hdfs.fileType = DataStream
storage.sinks.hdfs.channel      = channel1

storage.sinks.elasticsearch.type         = org.apache.flume.sink.elasticsearch.ElasticSearchSink
storage.sinks.elasticsearch.indexType    = bar_type
storage.sinks.elasticsearch.batchSize = 100
storage.sinks.elasticsearch.indexName = logstash
storage.sinks.elasticsearch.clusterName = elasticsearch
storage.sinks.elasticsearch.serializer = org.apache.flume.sink.elasticsearch.ElasticSearchLogStashEventSerializer
storage.sinks.elasticsearch.channel      = channel1

